import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy as stats
import time
import traceback
from hyperopt import hp, fmin, tpe, Trials, pyll, STATUS_OK
from hyperopt.fmin import generate_trials_to_calculate
from functools import partial
from backtesting import Backtest, Strategy
from sklearn.preprocessing import RobustScaler
from scipy.stats import ttest_1samp, wilcoxon

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score

st.markdown(
    """
    <style>
    .stApp {
        background-color: #0b1220;
        color: #e5e7eb;
    }
    section[data-testid="stSidebar"] {
        background-color: #111827;
    }
    </style>
    """,
    unsafe_allow_html=True
)
st.title("üìà ·ª®ng d·ª•ng ph√¢n t√≠ch d·ªØ li·ªáu v√† x√¢y d·ª±ng m√¥ h√¨nh d·ª± ƒëo√°n gi√° Crypto")
page = st.sidebar.selectbox(
    "Ch·ªçn ch·ª©c nƒÉng",
    ["Load data v√† Th·ªëng k√™ chung", "EDA", "Model"]
)

if page == "Load data v√† Th·ªëng k√™ chung":
    st.header("üìä Load data v√† Th·ªëng k√™ chung")
    # Load data
    choice = st.radio(
        "Ch·ªçn ngu·ªìn d·ªØ li·ªáu",
        ["Sample s·∫µn c√≥", "Upload d·ªØ li·ªáu c·ªßa b·∫°n"]
    )
    if choice == "Sample s·∫µn c√≥":
        data = pd.read_csv("ETHUSDT.csv")
    else:
        """***T·ªÜP C·∫¶N C√ì C√ÅC C·ªòT: timestamp, open, high, low, close, volume theo th·ª© t·ª±.***"""
        data = st.file_uploader("Upload your CSV file", type=["csv", "xlsx"])
        if data is not None:
            if data.name.endswith('.csv'):
                data = pd.read_csv(data)
            elif data.name.endswith('.xlsx'):
                data = pd.read_excel(data)

    if data is not None:
        try:
            st.subheader("üîç Ki·ªÉm tra ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu")
            st.write(f'**S·ªë h√†ng tr√πng l·∫Øp:**')
            st.write(f'**{data.duplicated().sum()}** h√†ng')
            data.rename(columns={'timestamp': 'Timestamp', 'open': 'Open', 'high': 'High', 'low': 'Low', 'close': 'Close', 'volume': 'Volume' }, inplace=True)
            data['Timestamp'] = pd.to_datetime(data['Timestamp'])
            data.set_index('Timestamp', inplace=True)
            data.sort_index(inplace=True)
            dup_ts = data.index.duplicated().sum()
            st.write(f"**S·ªë timestamp tr√πng l·∫∑p:**")
            st.write(f"{dup_ts}")

            st.subheader('Th√¥ng tin chung')
            st.write(f"#### Data Shape:")
            st.write(f"**{data.shape[0]} rows**, **{data.shape[1]} columns**")
            st.write(f"#### Date range:")
            st.write(f"**{data.index.min()}** to **{data.index.max()}**")
            st.write("#### Missing Values")
            st.dataframe(data.isna().sum().to_frame("Missing Count")) # D√πng to_frame ƒë·ªÉ hi·ªÉn v√¨ output l√† series
            st.write(F"#### Th·ªëng k√™ m√¥ t·∫£")                             
            st.dataframe(data.describe().T) # D√ông T v√¨ output l√† dataframe

        except Exception as e:
            st.error(f"Vui l√≤ng ƒë·∫∑t t√™n c·ªôt ƒë√∫ng ƒë·ªãnh d·∫°ng, th·ª© t·ª±: timestamp, open, high, low, close, volume")
            st.stop()

    # L∆∞u data d√πng chung
    st.session_state['data'] = data
    

elif page == "EDA":
    st.header("üìà EDA")

    data = st.session_state.data
    if data is None:
        st.warning("Vui l√≤ng t·∫£i d·ªØ li·ªáu trong m·ª•c 'Th·ªëng k√™ chung' tr∆∞·ªõc khi s·ª≠ d·ª•ng ch·ª©c nƒÉng n√†y.")
        st.stop()

    # T√≠nh, tr·ª±c sai ph√¢n b·∫≠c 1 gi√° ƒë√≥ng c·ª≠a
    diff = data['Close'].diff()
    window = 240
    min_periods = 100
    
    rolling_mean = diff.rolling(window, min_periods=min_periods).mean()
    rolling_std  = diff.rolling(window, min_periods=min_periods).std()

    st.subheader('Xu h∆∞·ªõng c·ª•c b·ªô v√† m·ª©c ƒë·ªô bi·∫øn ƒë·ªông c·ªßa ETH (30 ph√∫t)')
    fig, ax = plt.subplots(2, 1, figsize=(20,10), sharex=True)

    # Rolling mean
    ax[0].plot(rolling_mean, label='Rolling Mean (Diff)')
    ax[0].axhline(0, linestyle='--', alpha=0.5)
    ax[0].set_title('Rolling Mean of First-order Differencing')
    ax[0].legend()

    # Rolling std
    ax[1].plot(rolling_std, label='Rolling Std (Volatility)')
    ax[1].set_title('Rolling Volatility (Std of Diff)')
    ax[1].legend()

    st.pyplot(fig)

    close_diff = diff

    rolling_mean = close_diff.rolling(window=window, min_periods=min_periods).mean()
    rolling_std = close_diff.rolling(window=window, min_periods=min_periods).std()

    z_scores = (close_diff - rolling_mean)/rolling_std

    thresold = 3
    thresold1 = 4
    thresold2 = 5
    data_diff_filtered = close_diff.copy()
    mask = np.abs(z_scores) > thresold
    mask1 = np.abs(z_scores) > thresold1
    mask2 = np.abs(z_scores) > thresold2

    len_diff_greater3 = len(data_diff_filtered.loc[mask])
    rate_diff_greater3 = round(len_diff_greater3 / len(close_diff) * 100,2)
    def rate(rate):
        if rate <= 5:
            return 'Th·∫•p'
        elif 5 < rate < 10:
            return 'Trung b√¨nh'
        else:
            return 'Cao'
    st.markdown(f"""
    **T√≥m t·∫Øt th·ªëng k√™ (sai ph√¢n b·∫≠c 1):**
    - Trung b√¨nh l·ª£i su·∫•t: **{diff.mean()*100:.4f}%**
    - ƒê·ªô l·ªách chu·∫©n l·ª£i su·∫•t: **{diff.std():.2f}**
    - ƒê·ªô bi·∫øn ƒë·ªông l·ªõn nh·∫•t (rolling std): **{rolling_std.max():.2f}**
    - Sai ph√¢n b·∫≠c 1 gi√° ƒë√≥ng c·ª≠a v∆∞·ª£t ng∆∞·ª°ng **{thresold}**: **{len_diff_greater3}** (g·∫ßn **{rate_diff_greater3}%** l∆∞·ª£ng data) -> ***{rate(rate_diff_greater3)}***
    """)

    return_dist = close_diff

    # V·∫Ω bi·ªÉu ƒë·ªì Histogram
    st.subheader('Bi·ªÉu ƒë·ªì Histogram ph√¢n ph·ªëi l·ª£i nhu·∫≠n theo t·∫ßn su·∫•t')
    plt.figure(figsize=(10,5))
    plt.hist(return_dist, bins=100)
    plt.title(f"Bi·ªÉu ƒë·ªì Histogram th·ªÉ hi·ªán ph√¢n ph·ªëi l·ª£i nhu·∫≠n theo t·∫ßn su·∫•t\nSkew = {return_dist.skew():.2f}")
    plt.xlabel("Return")
    plt.ylabel("Frequency")
    st.pyplot(plt)
    def rate_iqr(iqr):
        if iqr < 0.005:
            return "Bi·∫øn ƒë·ªông th·∫•p, ph√¢n ph·ªëi t·∫≠p trung"
        elif iqr < 0.015:
            return "Bi·∫øn ƒë·ªông trung b√¨nh"
        else:
            return "Bi·∫øn ƒë·ªông cao, r·ªßi ro ƒëu√¥i l·ªõn"

    def rate_q1(q1):
        if q1 > -0.002:
            return "ƒêu√¥i √¢m n√¥ng, downside risk th·∫•p"
        elif q1 > -0.01:
            return "Downside risk trung b√¨nh"
        else:
            return "ƒêu√¥i √¢m d√†y, r·ªßi ro gi·∫£m m·∫°nh"
    def rate_q3(q3):
        if q3 < 0.002:
            return "Bi√™n ƒë·ªô tƒÉng y·∫øu"
        elif q3 < 0.01:
            return "Upside trung b√¨nh"
        else:
            return "Upside m·∫°nh, xu·∫•t hi·ªán c√°c nh·ªãp tƒÉng l·ªõn"
    q1 = return_dist.quantile(0.25)
    q3 = return_dist.quantile(0.75)
    iqr = q3 - q1

    st.markdown(f"""
    **R·ªßi ro & c∆° h·ªôi ƒëu√¥i ph√¢n ph·ªëi (l·ª£i su·∫•t):**
    - Q1 (25%): **{q1:.2f}** ‚Üí *{rate_q1(q1)}*
    - Q3 (75%): **{q3:.2f}** ‚Üí *{rate_q3(q3)}*
    - IQR: **{iqr:.2f}** ‚Üí *{rate_iqr(iqr)}*
    """)

    # V·∫Ω bi·ªÉu ƒë·ªì Boxplot
    st.subheader('Bi·ªÉu ƒë·ªì th·ªÉ hi·ªán gi√° tr·ªã ngo·∫°i lai c·ªßa l·ª£i nhu·∫≠n')
    plt.figure(figsize=(6,5))
    plt.boxplot(return_dist[1:], vert=True)
    plt.title(f"Bi·ªÉu ƒë·ªì boxplot xem c√°c gi√° tr·ªã ngo·∫°i lai c·ªßa l·ª£i nhu·∫≠n\nKurtosis = {return_dist.kurtosis():.2f}")
    plt.ylabel("Return")
    st.pyplot(plt)
    def rate_kurtosis(kurt):
        if kurt < 0:
            return "Ph√¢n ph·ªëi b·∫πt, √≠t ƒëu√¥i d√†y, tail risk th·∫•p"
        elif kurt < 3:
            return "Ph√¢n ph·ªëi g·∫ßn chu·∫©n, tail risk trung b√¨nh"
        elif kurt < 7:
            return "Ph√¢n ph·ªëi ƒëu√¥i d√†y, th∆∞·ªùng xuy√™n xu·∫•t hi·ªán bi·∫øn ƒë·ªông l·ªõn"
        else:
            return "ƒêu√¥i r·∫•t d√†y, r·ªßi ro c·ª±c ƒëoan cao (fat-tail risk)"
    def rate_outlier_ratio(outlier_ratio):
        if outlier_ratio < 1:
            return "R·∫•t √≠t ngo·∫°i lai, bi·∫øn ƒë·ªông t∆∞∆°ng ƒë·ªëi ·ªïn ƒë·ªãnh"
        elif outlier_ratio < 5:
            return "C√≥ m·ªôt s·ªë ngo·∫°i lai, xu·∫•t hi·ªán shock ng·∫Øn h·∫°n"
        elif outlier_ratio < 10:
            return "Nhi·ªÅu ngo·∫°i lai, th·ªã tr∆∞·ªùng bi·∫øn ƒë·ªông m·∫°nh"
        else:
            return "Ngo·∫°i lai d√†y ƒë·∫∑c, r·ªßi ro bi·∫øn ƒë·ªông c·ª±c ƒëoan"
    q1 = return_dist.quantile(0.25)
    q3 = return_dist.quantile(0.75)
    iqr = q3 - q1

    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr

    outliers = return_dist[(return_dist < lower_bound) | (return_dist > upper_bound)]
    outlier_ratio = len(outliers) / len(return_dist) * 100
    kurt = return_dist.kurtosis()

    st.markdown(f"""
    **Ph√¢n t√≠ch ngo·∫°i lai & tail risk (Boxplot):**
    - Kurtosis: **{kurt:.2f}** ‚Üí *{rate_kurtosis(kurt)}*
    - T·ª∑ l·ªá ngo·∫°i lai: **{outlier_ratio:.2f}%** ‚Üí *{rate_outlier_ratio(outlier_ratio)}*
    - Bi√™n d∆∞·ªõi (Lower bound): **{lower_bound:.2f}**
    - Bi√™n tr√™n (Upper bound): **{upper_bound:.2f}**

    Boxplot cho th·∫•y s·ª± xu·∫•t hi·ªán c·ªßa nhi·ªÅu gi√° tr·ªã ngo·∫°i lai, ph·∫£n √°nh c√°c c√∫ bi·∫øn ƒë·ªông b·∫•t th∆∞·ªùng trong l·ª£i su·∫•t.
    """)





elif page == "Model":
    st.header("üìà X√¢y d·ª±ng m√¥ h√¨nh d·ª± ƒëo√°n xu h∆∞·ªõng b·∫±ng thu·∫≠t to√°n RandomForest")

    data = st.session_state.data
    if data is None:
        st.warning("Vui l√≤ng t·∫£i d·ªØ li·ªáu trong m·ª•c 'Th·ªëng k√™ chung' tr∆∞·ªõc khi s·ª≠ d·ª•ng ch·ª©c nƒÉng n√†y.")
        st.stop()
    # Chu·∫©n b·ªã c√°c bi·∫øn ƒë·∫∑c tr∆∞ng c∆° b·∫£n
    df = data.copy()
    df['Return'] = df['Close'].pct_change()

    df['Daily'] = df.index.day
    df['Weekday'] = df.index.weekday

    df['Return_2'] = df['Return'].shift(2)

    df['Upper_shadow'] = (df['High'] - df['Close']) / df['Close']
    df['Lower_shadow'] = (df['Close'] - df['Low']) / df['Close']

    df['High_gap'] = (df['High'].shift(1) - df['Close']) / df['Close']
    df['Low_gap'] = (df['Close'] - df['Low'].shift(1)) / df['Close']

    df['vp_ratio_1'] = (df['Volume'].shift(1)) / (df['Close'].shift(1))

    features = ['Daily','Weekday','Return_2',
            'Upper_shadow','Lower_shadow',
            'High_gap', 'Low_gap',
            'vp_ratio_1','Return']

    corr = df[features].corr()

    st.subheader("Bi·ªÉu ƒë·ªì t∆∞∆°ng quan c√°c bi·∫øn c∆° b·∫£n")
    plt.figure(figsize=(10,8))
    sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f")
    plt.title("T∆∞∆°ng quan c√°c bi·∫øn c∆° b·∫£n vs l·ª£i nhu·∫≠n")
    st.pyplot(plt)

    # T√¨m c√°c ƒë·∫∑c tr∆∞ng c√≥ t∆∞∆°ng quan th·∫•p so v·ªõi return
    low_corr = corr['Return'].abs() < 0.05
    remove_low_corr = list(low_corr[low_corr].index)
    df = df.drop(remove_low_corr, axis=1)
    st.write(f'ƒê√£ lo·∫°i b·ªè c√°c bi·∫øn c∆° b·∫£n c√≥ t∆∞∆°ng quan th·∫•p v·ªõi Return: {remove_low_corr}')

    # Chu·∫©n b·ªã c√°c bi·∫øn ƒë·∫∑c tr∆∞ng ch·ªâ b√°o 
    #1. RSI
    window = 14
    delta = df['Close'].diff()

    gain = np.where(delta > 0, delta, 0)
    loss = np.where(delta < 0, -delta, 0)

    # gi·ªØ nguy√™n index c·ªßa df
    gain = pd.Series(gain, index=df.index)
    loss = pd.Series(loss, index=df.index)

    avg_gain = gain.rolling(14).mean()
    avg_loss = loss.rolling(14).mean()

    rs = avg_gain / avg_loss
    df['RSI14'] = 100 - (100 / (1 + rs))

    # 2. MACD 
    ema6 = df['Close'].ewm(span=6, adjust=False).mean()
    ema20 = df['Close'].ewm(span=20, adjust=False).mean()

    df['MACD'] = ema6 - ema20
    df['Signal'] = df['MACD'].ewm(span=4, adjust=False).mean()
    df['MACD_HIST'] = df['MACD'] - df['Signal']

    # 3. ATR(14) normalized
    high_low = df['High'] - df['Low']
    high_close = abs(df['High'] - df['Close'].shift(1))
    low_close = abs(df['Low'] - df['Close'].shift(1))

    tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
    df['ATR14'] = tr.ewm(span=14, adjust=False).mean()
    df['ATR_NORM'] = df['ATR14'] / df['Close']

    # 4. Volume Spike Relative Ratio

    df['Vol_shock'] = (df['Volume'] - df['Volume'].rolling(4).mean()) / df['Volume'].rolling(48).std()

    # 5. Spike-Weighted ATR
    df['TR'] = np.maximum(df['High'] - df['Low'],
                    np.maximum(abs(df['High'] - df['Close'].shift(1)),
                            abs(df['Low'] - df['Close'].shift(1))))

    # ATR c∆° b·∫£n
    df['ATR4'] = df['TR'].rolling(4).mean()

    # Spike Factor = TR hi·ªán t·∫°i cao bao nhi√™u l·∫ßn so v·ªõi TR trung b√¨nh
    df['Spike_factor'] = df['TR'] / df['TR'].rolling(48).mean()

    # Spike-Weighted ATR
    df['SW_ATR'] = df['ATR4'] * df['Spike_factor']

    df.drop(columns=['MACD', 'Signal', 'ATR14', 'TR', 'ATR4', 'Spike_factor'], errors='ignore', inplace=True)

    indicator_features = ['RSI14', 'MACD_HIST', 'ATR_NORM', 'Vol_shock', 'SW_ATR', 'Return']
    indicator_features_corr = df[indicator_features].corr()

    st.subheader("Bi·ªÉu ƒë·ªì t∆∞∆°ng quan c√°c bi·∫øn ch·ªâ b√°o k·ªπ thu·∫≠t")
    plt.figure(figsize=(10,8))
    sns.heatmap(indicator_features_corr, annot=True, cmap="coolwarm", fmt=".2f")
    plt.title("T∆∞∆°ng quan c√°c bi·∫øn ch·ªâ b√°o k·ªπ thu·∫≠t vs l·ª£i nhu·∫≠n")
    st.pyplot(plt)
    # T√¨m c√°c ƒë·∫∑c tr∆∞ng c√≥ t∆∞∆°ng quan th·∫•p so v·ªõi return
    low_indicator_corr = indicator_features_corr['Return'].abs() < 0.05
    remove_low_indicator_corr = list(low_indicator_corr[low_indicator_corr].index)
    df = df.drop(remove_low_indicator_corr, axis=1)
    st.write(f'ƒê√£ lo·∫°i b·ªè c√°c bi·∫øn ch·ªâ b√°o k·ªπ thu·∫≠t c√≥ t∆∞∆°ng quan th·∫•p v·ªõi Return: {remove_low_indicator_corr}')

    df = df.dropna()

    st.subheader("B·∫Øt ƒë·∫ßu chu·∫©n h√≥a d·ªØ li·ªáu v·ªõi RobustScaler")
    # Ch·ªçn c√°c ƒë·∫∑c tr∆∞ng hu·∫•n luy·ªán cho m√¥ h√¨nh nh·∫±m d·ª± b√°o
    features = ['Upper_shadow', 'Lower_shadow', 'High_gap', 'Low_gap',
                'RSI14', 'MACD_HIST', 'SW_ATR', 'Vol_shock']

    # Chu·∫©n h√≥a theo RobusScaler
    scaler = RobustScaler()
    df[features] = scaler.fit_transform(df[features])
    st.write("D·ªØ li·ªáu sau khi chu·∫©n h√≥a:")
    st.dataframe(df[features].describe().loc[['max', 'min', 'mean']])

    # H√†m chi·∫øn l∆∞·ª£c mua b√°n
    class GeneralStrategy(Strategy):
        def init(self):
            pass
        def next(self):
            signal = self.data.position[-1]
            if signal == 1:
                # M·ªü long if signal = 1:
                if not self.position:
                    # ƒëang ƒë·ª©ng ngo√†i -> mua v√†o
                    self.buy() # ƒê√≥ng long
            elif signal == -1:
                if self.position.is_long:
                    # ƒëang c·∫ßm long -> ƒë√≥ng
                    self.position.close()
                    # signal = 0 > ƒë·ª©ng ngo√†i, kh√¥ng l√†m g√¨
    
    
    # H√†m t√≠nh Sharpe Ratio
    def compute_sharpe(returns, window=17520):
        try:
            return np.sqrt(window) * (returns.mean() / returns.std())
        except:
            return np.nan
    
    # H√†m sinh v·ªã th·∫ø v·ªã th·∫ø giao d·ªãch
    def find_position(df, paras):
        """H√†m t√¨m v·ªã th·∫ø giao d·ªãch"""
        # Ch√®n tham s·ªë
        try:
            n_estimators = int(paras["n_estimators"])
            max_depth = int(paras["max_depth"])
            min_samples_split = int(paras["min_samples_split"])
            min_samples_leaf = int(paras["min_samples_leaf"])
        except:
            n_estimators = int(paras[0])
            max_depth = int(paras[1])
            min_samples_split = int(paras[2])
            min_samples_leaf = int(paras[3])

        df_rf = df.copy()

        # T·∫°o nh√£n xu h∆∞·ªõng (Trend)
        threshold = 0.001  # 0.1% return
        df_rf["Trend"] = df_rf["Close"].shift(-1)/df_rf["Close"] - 1
        df_rf["Trend"] = df_rf["Trend"].apply(lambda x: 1 if x > threshold else -1)
        df_rf = df_rf.dropna(subset=["Trend"])

        #  Split data (75% train, 25% test)
        train_size = int(len(df_rf) * 0.75)
        df_train = df_rf.iloc[:train_size]


        # C√°c feature ƒë∆∞·ª£c ch·ªçn ƒë·ªÉ ch·∫°y model
        features = ['Upper_shadow','Lower_shadow','High_gap','Low_gap',
                    'RSI14','MACD_HIST','SW_ATR','Vol_shock']

        X_train = df_train[features]
        y_train = df_train["Trend"]

        # Train model
        model = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            random_state=42,
            n_jobs=-1
        )
        model.fit(X_train, y_train)

        # D·ª± ƒëo√°n x√°c su·∫•t c·ªßa t·ª´ng l·ªõp (tƒÉng/gi·∫£m) cho m·ªói quan s√°t
        proba = model.predict_proba(df_rf[features])
        classes = model.classes_

        idx_up = np.where(classes == 1)[0][0]     # T√¨m v·ªã tr√≠ tƒÉng
        idx_down = np.where(classes == -1)[0][0]  # T√¨m v·ªã tr√≠ gi·∫£m

        prob_up = proba[:, idx_up]                # X√°c su·∫•t tƒÉng
        prob_down = proba[:, idx_down]            # X√°c su·∫•t l·ªá gi·∫£m

        # T√≠nh delta: ƒë·ªô ch√™nh gi·ªØa x√°c su·∫•t tƒÉng v√† gi·∫£m
        # Delta d∆∞∆°ng -> k·ª≥ v·ªçng tƒÉng, Delta √¢m -> k·ª≥ v·ªçng gi·∫£m
        delta = prob_up - prob_down
        delta = pd.Series(delta, index=df_rf.index)

        # L√†m m∆∞·ª£t delta b·∫±ng EMA (Exponential Moving Average) v·ªõi span=48
        # Gi√∫p gi·∫£m nhi·ªÖu v√† t·∫°o t√≠n hi·ªáu ·ªïn ƒë·ªãnh h∆°n cho vi·ªác v√†o/tho√°t l·ªánh
        delta_smooth = delta.ewm(span=8, adjust=False).mean()

        # Ng∆∞·ª°ng k√¨ v·ªçng
        entry_thr = 0.06  # Ng∆∞·ª°ng k·ª≥ v·ªçng ƒë·ªÉ v√†o l·ªánh mua (delta_smooth v∆∞·ª£t m·ª©c n√†y)
        exit_thr = -0.06 # Ng∆∞·ª°ng k·ª≥ v·ªçng ƒë·ªÉ tho√°t l·ªánh/b√°n (delta_smooth d∆∞·ªõi m·ª©c n√†y)

        # T·∫°o bi·∫øn v·ªã th·∫ø
        pos_event = pd.Series(0.0, index=df_rf.index)

        state = 0         # T√¨nh tr·∫°ng (ch∆∞a v√†o l·ªánh: 0, ƒëang gi·ªØ l·ªánh: 1)
        hold = 0          # S·ªë phi√™n ƒë√£ gi·ªØ l·ªánh
        min_hold = 0     # S·ªë phi√™n t·ªëi thi·ªÉu c·∫ßn gi·ªØ l·ªánh

        # Duy·ªát qua t·ª´ng th·ªùi ƒëi·ªÉm ƒë·ªÉ x√°c ƒë·ªãnh t√≠n hi·ªáu giao d·ªãch
        for t in df_rf.index:
            d = delta_smooth.at[t]

            # N·∫øu ch∆∞a v√†o l·ªánh
            if state == 0:
                if d > entry_thr:           # N·∫øu k·ª≥ v·ªçng tƒÉng v∆∞·ª£t ng∆∞·ª°ng -> v√†o l·ªánh mua
                    pos_event.at[t] = 1.0
                    state = 1
                    hold = 0

            elif state == 1:
                hold += 1
                if (d < exit_thr) and hold >= min_hold: # N·∫øu k·ª≥ v·ªçng gi·∫£m v∆∞·ª£t ng∆∞·ª°ng v√† ƒë√£ gi·ªØ ƒë·ªß phi√™n ‚Üí tho√°t l·ªánh
                    pos_event.at[t] = -1.0
                    state = 0
                    hold = 0

        # ƒê·ªÉ tr√°nh vi·ªác d√πng d·ªØ li·ªáu t∆∞∆°ng lai (look-ahead bias)
        pos_event = pos_event.shift(1).fillna(0.0)

        # ƒê·∫£m b·∫£o kh·ªõp index c·ªßa df g·ªëc
        pos_full = pd.Series(0.0, index=df.index)
        pos_full.loc[pos_event.index] = pos_event

        return pos_full

    # H√†m backtesting
    def backtest(df, cash, commission, print_result=True):
        bt = Backtest(df,
                        GeneralStrategy,
                        cash=cash,
                        commission=commission,
                        trade_on_close=False,
                        exclusive_orders=True)

        stats = bt.run()
        stats = stats.to_frame()

        # T√≠nh sharpe ratio
        stats.loc['Duration'] = len(df)
        returns = stats.loc['_equity_curve'][0]['Equity'].pct_change() * 100
        stats.loc['Sharpe Ratio'] = compute_sharpe(returns=returns, window=len(df))

        # L√†m tr√≤n s·ªë
        stats = stats.round(2)

        if print_result:
            st.write("==== Backtest Results ====")
            st.write(f"Return: {stats.loc['Return [%]'][0]}%")
            st.write(f"Sharpe Ratio: {stats.loc['Sharpe Ratio'][0]}")
            st.write(f"Max Drawdown: {stats.loc['Max. Drawdown [%]'][0]}%")
            st.write(f"CAGR: {stats.loc['Return (Ann.) [%]'][0]}%")
            st.write(f"Win Rate: {stats.loc['Win Rate [%]'][0]}%")
            st.write(f"Total Trades: {int(stats.loc['# Trades'][0])}")
            st.write(f"Profit Factor: {stats.loc['Profit Factor'][0]}")
            st.write("==========================")

        return bt, stats
    
    # H√†m ƒë√°nh gi√° hi·ªáu su·∫•t m√¥ h√¨nh
    def score(paras, df, cash, commission, fitness, print_result=False):

        # Hu·∫•n luy·ªán model
        position_series = find_position(df, paras)

        # G√°n t√≠n hi·ªáu v√†o c·ªôt 'position'
        df['position'] = position_series

        # G·ªçi backtest
        bt, stats = backtest(df, cash, commission, print_result=print_result)

        if fitness == 'combo':
            sharpe = stats.loc['Sharpe Ratio'][0]
            cagr = stats.loc['Return (Ann.) [%]'][0]
            res = -(sharpe + 0.1 * cagr)              # 0.1 v√¨ cagr c√≥ ƒë∆°n v·ªã %
        elif fitness == 'sharpe':
            res = -stats.loc['Sharpe Ratio'][0]
        elif fitness == 'cagr':
            res = -stats.loc['Return (Ann.) [%]'][0]
        return res

    fspace = {
        'n_estimators': hp.quniform('n_estimators', 100, 500, 5), # S·ªë c√¢y trong r·ª´ng
        'max_depth': hp.quniform('max_depth', 10, 35, 1), # ƒê·ªô s√¢u t·ªëi ƒëa c·ªßa c√¢y
        'min_samples_split': hp.quniform('min_samples_split', 5, 70, 1), # S·ªë l∆∞·ª£ng m·∫´u t·ªëi thi·ªÉu ƒë·ªÉ chia m·ªôt node
        'min_samples_leaf': hp.quniform('min_samples_leaf', 5, 70, 1) # S·ªë l∆∞·ª£ng m·∫´u t·ªëi thi·ªÉu t·∫°i node l√°
    }

    cash = 1000000
    commission = 0.001
    max_evals = 20
    fitness = 'cagr'
    df_rf = df.copy()
    paras_best = None
    
    st.header("üöÄ Tuning Parasmeter")
    mode1 = st.radio(
        "",
        ["S·ª≠ d·ª•ng tham s·ªë h·ªá th·ªëng (nhanh)", "Tuning Parasmeter (ch·∫≠m)"]
    )

    if mode1 == "S·ª≠ d·ª•ng tham s·ªë h·ªá th·ªëng (nhanh)":
        if st.button("S·ª≠ d·ª•ng tham s·ªë ƒë√£ ƒë∆∞·ª£c ch·ªçn tr∆∞·ªõc cho Random Forest"):
            paras_best = {
                "max_depth" : 22,
                "min_samples_leaf" : 23,
                "min_samples_split" : 42,
                "n_estimators" : 120
            }
    else:
        # Parameters tuning
        if st.button("üöÄ B·∫Øt ƒë·∫ßu qu√° tr√¨nh Tuning"):
            # T·∫°o dict ch·ª©a kh√¥ng gian tham s·ªë
            fmin_objective = partial(score, df=df_rf, cash=cash, commission=commission, fitness=fitness)
            # T·ªëi ∆∞u h√≥a tham s·ªë
            st.write("Running Parameter Tuning for Random Forest...")
            paras_best = fmin(fn=fmin_objective, space=fspace, algo=tpe.suggest, max_evals=max_evals)
            st.write("Best Parameters:", paras_best)   
         
    if paras_best is not None:
        # Backtest v·ªõi tham s·ªë t·ªët nh·∫•t
        st.write('\nBacktest on Test Data (with Best RF Parameters):')
        # L·∫•y n·ª≠a sau ƒë·ªÉ test tham s·ªë v·ª´a t√¨m ƒë∆∞·ª£c
        df_rf_test = df_rf.copy()

        position_test = find_position(df_rf_test, paras_best)

        df_rf_test['position'] = position_test

        # Th·ª±c hi·ªán backtest
        bt_test, stats_test = backtest(df_rf_test, cash, commission)
        bt_test.plot(plot_width = 900, plot_volume=False, superimpose=False)



    st.header("üöÄ Walk-Forward Optimization")

    # Walk - Forward Optimization
    cash = 1000000
    commission = 0.001
    max_evals = 15
    fitness = 'cagr'
    num_split = 5

    # T·∫°o h√†m chia d·ªØ li·ªáu theo seq
    def split_data(df_split, seq):
        df_train = pd.concat([df_split[seq], df_split[seq+1]])
        df_test = pd.concat([df_split[seq+2], df_split[seq+3]])

        return df_train, df_test

    # T·∫°o h√†m in hi·ªáu su·∫•t
    def print_perf(period, fitness, loss):
        perf_df = pd.DataFrame()
        perf_df[f'{fitness}_{period}'] = [-round(loss, 2)]

        return perf_df

    # T·∫°o h√†m ch·ª©a s·ªë m·∫∑c ƒë·ªãnh cho RF-model
    def init_paras():
        return {
                'n_estimators': 50,
                'max_depth': 10,
                'min_samples_split': 5,
                'min_samples_leaf': 2
            }

    def mean_ttest(df, mean_val, col=None):
        """H√†m ƒëo hi·ªáu su·∫•t ki·ªÉm ƒë·ªãnh t-test"""
        if col is None:
            statistic, pval = ttest_1samp(df, mean_val, alternative='greater')
        else:
            statistic, pval = ttest_1samp(df[col], mean_val, alternative='greater')

        # Ki·ªÉm ƒë·ªãnh pvalue v·ªõi gi√° tr·ªã 10%
        m1_greater_m2 = pval < 0.1

        # T·∫°o dict ch·ª© k·∫øt qu·∫£
        results = {
            "pvalue": pval,
            f"\n{df} greater {mean_val}": m1_greater_m2,
        }

        return results

    def compare_means_ttest(df1, df2, col1=None, col2=None):
        """H√†m ƒëo hi·ªáu su·∫•t ki·ªÉm ƒë·ªãnh wilcoxon"""
        if col1 is None or col2 is None:
            statistic, pval = wilcoxon(df1, df2, alternative='greater')
        else:
            statistic, pval = wilcoxon(df1[col1], df2[col2], alternative='greater')

        # Ki·ªÉm ƒë·ªãnh pvalue v·ªõi gi√° tr·ªã 10%
        m1_greater_m2 = pval < 0.1

        # T·∫°o dict ch·ª©a k·∫øt qu·∫£ ki·ªÉm ƒë·ªãnh
        results = {
            "pvalue": pval,
            f"\n{df1} greater than {df2}": m1_greater_m2,
        }

        return results

    if st.button("üöÄ B·∫Øt ƒë·∫ßu qu√° tr√¨nh Walk-Forward Optimization"):
        """V√¨ qu√° tr√¨nh walk-forward kh√° l√¢u n√™n ti·∫øt ki·ªám th·ªùi gian s·∫Ω ch·∫°y v·ªõi s·ªë l∆∞·ª£ng num split =  6"""
        time_start = time.time()

        perf_total_train = pd.DataFrame()
        perf_total_test = pd.DataFrame()

        # Chia d·ªØ li·ªáu th√†nh c√°c ph·∫ßn
        df_split = np.array_split(df[:int(len(df_rf)*0.7)].copy(), num_split)
        paras_default = init_paras()

        st.write("\n B·∫Øt ƒë·∫ßu qu√° tr√¨nh Walk-Forward \n")

        fmin_objective_train = partial(score, cash=cash,
                                    commission=commission, fitness=fitness)
        # Walk-Forward loop
        for seq in range(len(df_split) - 3):
            df_train, df_test = split_data(df_split, seq)
            seq += 1
            #TRAIN
            try:
                trials = generate_trials_to_calculate([init_paras()])
                paras_train = fmin(fn=partial(score, df=df_train, cash=cash, commission=commission, fitness=fitness),
                                    space=fspace, algo=tpe.suggest, max_evals=max_evals, trials=trials, show_progressbar=False)
                loss_train = score(paras_train, df_train, cash, commission, fitness)

            except Exception as e:
                st.write(f"L·ªói trong qu√° tr√¨nh t·ªëi ∆∞u h√≥a trainning: {e}")
                traceback.print_exc()
                paras_train = paras_default
                loss_train = score(paras_train, df_train, cash,
                                            commission, fitness, print_result=False)

            # T√≠nh loss v·ªõi tham s·ªë m·∫∑c ƒë·ªãnh
            loss_train_default = score(paras_default, df_train, cash,
                                        commission, fitness)
            st.write(f"""
                ### [{seq}] Train RF
                - **Ng√†y:** {df_train.index[0]}
                - **Tham s·ªë train:** {paras_train}
                - **Sai s·ªë:** {-loss_train}
                """
                )

            # TEST
            try:
                paras_test = paras_train
                loss_test = score(paras_test, df_test, cash, commission, fitness)
                loss_test_default = score(paras_default, df_test, cash, commission, fitness)

                st.write(f"""
                        ### [{seq}] Test RF 
                        - **Ng√†y - gi·ªù:** {df_test.index[0]} 
                        - **Sai s·ªë (tham s·ªë t·ªëi ∆∞u) tr√™n t·∫≠p test:** {-loss_test} 
                        - **Sai s·ªë (tham s·ªë m·∫∑c ƒë·ªãnh) tr√™n t·∫≠p test:** {loss_test_default}
                """)

                perf_train = print_perf('Train_RF', fitness, loss_train)
                perf_test = print_perf('Test_RF', fitness, loss_test)

                perf_train_default = print_perf('Train_RF_default', fitness, loss_train_default)
                perf_test_default = print_perf('Test_RF_default', fitness, loss_test_default)

                temp_total_train = pd.concat([perf_train, perf_train_default], axis=1)
                temp_total_test = pd.concat([perf_test, perf_test_default], axis=1)

                perf_total_train = pd.concat([perf_total_train, temp_total_train])
                perf_total_test = pd.concat([perf_total_test, temp_total_test])

            except Exception as e:
                st.write('L·ªói trong qu√° tr√¨nh testing ·ªü gian ƒëo·∫°n th·ª© {}'.format(seq))
                traceback.print_exc()
                perf_total_train_rf = pd.concat([perf_total_train_rf, pd.DataFrame(np.nan, index=[0], columns=['Return_train_RF', 'Return_train_default_RF'])])
                perf_total_test_rf = pd.concat([perf_total_test_rf, pd.DataFrame(np.nan, index=[0], columns=['Return_test_RF', 'Return_test_default_RF'])])

        st.subheader("ƒê√°nh gi√° m√¥ h√¨nh sau khi Walk-Forward Optimization")
        st.write(f"Best paras: {paras_train}")
        threshold = 0.001
        df_rf["Trend"] = df_rf["Close"].shift(-1)/df_rf["Close"] - 1
        df_rf["Trend"] = df_rf["Trend"].apply(lambda x: 1 if x > threshold else -1)
        df_rf = df_rf.dropna(subset=["Trend"])

        X = df_rf[features]
        y = df_rf['Trend']

        model = RandomForestClassifier(
            n_estimators=int(paras_train['n_estimators']),
            max_depth=int(paras_train['max_depth']),
            min_samples_split=int(paras_train['min_samples_split']),
            min_samples_leaf=int(paras_train['min_samples_leaf']),
            random_state=42
        )

        model.fit(X, y)
        y_pred = model.predict(X)

        # ƒê√°nh gi√°
        st.subheader("=== Evaluation on Final Walk-Forward Test Segment ===")
        st.write(confusion_matrix(y, y_pred))
        st.write(classification_report(y, y_pred))
        st.write("Accuracy:", accuracy_score(y, y_pred))
        st.write("F1 Score:", f1_score(y, y_pred, average='weighted'))

        st.write(f"\nLength of train data: {len(df_train)}, Length of test data: {len(df_test)}")
        st.write("\nMean Performance (Train - RF):")
        st.write(round(perf_total_train.mean(),2))
        st.write("\nMean Performance (Test - RF):")
        st.write(round(perf_total_test.mean(),2))
        st.write('Running time (RF WF): %.2fs -- %.2fm' % (time.time() - time_start, (time.time() - time_start)/60))


        perf_total_train.reset_index(drop=True, inplace=True)
        perf_total_train.index.names = ['Period']
        perf_total_test.reset_index(drop=True, inplace=True)
        perf_total_test.index.names = ['Period']

        st.subheader("\nPerformance Summary (RF WF):")
        summary_df = pd.concat([perf_total_train, perf_total_test], axis=1)
        st.dataframe(summary_df)


        st.subheader('Ki·ªÉm ƒë·ªãnh th·ªëng k√™ cho k·∫øt qu·∫£ Walk-Forward RF')

        """Ki·ªÉm ƒë·ªãnh t-test xem t·ª∑ su·∫•t l·ª£i nhu·∫≠n trung b√¨nh tr√™n t·∫≠p test (ƒë√£ t·ªëi ∆∞u) c√≥ l·ªõn h∆°n 0 kh√¥ng"""
        results_test_optimized = mean_ttest(perf_total_test.dropna(), 0, 'cagr_Test_RF')
        st.write(f"""
                 #### *RF Optimized Test Return Annual (vs 0):* 
                 {results_test_optimized}
                """)

        """Ki·ªÉm ƒë·ªãnh t-test xem t·ª∑ su·∫•t l·ª£i nhu·∫≠n trung b√¨nh tr√™n t·∫≠p test (m·∫∑c ƒë·ªãnh) c√≥ l·ªõn h∆°n 0 kh√¥ng"""
        results_test_default = mean_ttest(perf_total_test.dropna(), 0, 'cagr_Test_RF_default')
        st.write(f"""
                 #### *RF Default Test Return Annual (vs 0):* 
                 {results_test_default}
            """)

        """Ki·ªÉm ƒë·ªãnh Wilcoxon xem l·ª£i nhu·∫≠n trung b√¨nh tr√™n t·∫≠p test (t·ªëi ∆∞u) c√≥ l·ªõn h∆°n l·ª£i nhu·∫≠n (m·∫∑c ƒë·ªãnh) kh√¥ng"""
        results_test_comparison = compare_means_ttest(perf_total_test.dropna(), perf_total_test.dropna(), 'cagr_Test_RF', 'cagr_Test_RF_default')
        st.write(f"""
                 #### *RF Test Return Anuual (Optimized vs Default):*
                {results_test_comparison}
            """)

        # Ch·∫°y backtest tr√™n t·∫≠p ki·ªÉm ƒë·ªãnh
        df_ETH = df.copy()

        # L·∫•y t√≠n hi·ªáu giao d·ªãch t·ª´ model cu·ªëi
        position_series = find_position(df_ETH, paras_train)

        df_ETH['position'] = position_series

        df_ETH_test = df_ETH

        bt, stats = backtest(df_ETH_test, cash, commission)
        bt.plot(plot_width=900, plot_volume=False, superimpose=False)